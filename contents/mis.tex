\chapter{Multiple Importance Sampling}
\label{ch:mis}

\todo{Match to actual structure of this chapter}
In this chapter the fundamentals of Monte Carlo integration and importance sampling will be introduced.
After that Multiple Importance Sampling is explained which is the fundament for the next chapter \ref{ch:mis_compensation}.


\section{Probability Basics}
\label{sec:probability_basics}

For our use case we need to define random variables, probability distribution functions (pdf) and cumulative distribution functions (cdf).
A random variable maps an event to a real number $ X: \Omega \to \mathbb{R} $.
In a discrete scenario our random variable corresponds to one exact event e.g. a dice throw showing three pips on the top.
The probability for this can be expressed as $ P(X = 3) = \frac{1}{6} $ or as $ X(3) = \frac{1}{6} $
since the random variable $ X $ can take six different values which are all equally likely.
To express the probability that an event is within a range of values we can use the cdf
which is defined as $$ F(x) = P(X \leq x), x \in \mathbb{R}. $$
For the probability of a random variable in an interval $ [a, b) $ we can write $$ P(a \leq X < b) = F(b) - F(a). \cite{pris} $$

When we are talking about continuous events e.g. turning a wheel of fortune, every exact angle has a probability of zero.
But we can still express our probability for an angle interval with the cdf from above.
In the continuous case we have a pdf that is used to calculate the cdf with $$ F(x) = \int_{-\infty}^x p(\tilde{x}) d\tilde{x}. $$
The pdf has to be non-negative ($ \forall x: p(x) \geq 0 $) and integrate to one ($ \int_{-\infty}^{\infty} p(x) dx = 1$) to be valid and
\enquote{[...] describes the relative probability of a random variable taking on a particular value.}\cite[Chapter~13.1]{pbr-book}
\todo{How to cite when the content of this section was created with a reference?}


\section{Generating Samples after a specific Function}
\label{sec:sample_generation}
This will be very important for the next section \ref{sec:monte_carlo},
since this is our adjustment wheel to manipulate the quality of our rendering (for a given amount of time).
Most of the time we don't need evenly distributed samples,
rather our samples should follow a specific characteristic of a material or a scene.
For this we need to know how to generate samples that correspond to that characteristic.

Given a function $ f(x) $ after which we would like to draw samples the first step is to make sure
it fulfills the constraints for a pdf in an interval $ [a, b) $ we want to use.
First we check that $ \forall x \in [a, b): f(x) \geq 0 $ and then calculate the integral with
\begin{equation}
\label{eq:integral_fx}
    F = \int_{a}^b f(x) dx.
\end{equation}
$ F $ can be used to normalize our goal function which yields us $ \tilde{f}(x) = \frac{f(x)}{F} $ as a valid pdf.
The next step is to calculate our cdf as $ F(x) = \int_{a}^x \tilde{f}(t) dt $ which we will then invert to get $ F^{-1}(x) $.
Now we only need to draw an equally distributed random number (which most programming languages have a library for) in the interval $ [0, 1) $
which we call $ \xi $ and evaluate $ F^{-1}(\xi) = X $ to get $ X $ which is distributed after $ f(.) $.
This method is called "inverse cdf". \cite{pris}
\todo{Should I also cover 2D sampling?}
\todo{Should I cover more sampling methods (rejection sampling e.g.)}

\section{Monte Carlo Integration}
\label{sec:monte_carlo}

Monte Carlo integration is a technique to approximate the integral of an arbitrary function $ f(x) $
by taking $ N $ random samples $ X_i $ from a pdf $ p(X_i) $.
As we know the expectation of a random variable is calculated by $$ E(X) = \int_{-\infty}^\infty x * p(x) dx $$
or more generally $$ E(g(x)) = \int_{-\infty}^\infty g(x) * p(x) dx. $$
For the next step we need the law of large numbers which states that $$ P\left[ \lim_{N\to\infty} \frac{1}{N} \sum_{i = 0}^N X_i = E(X) \right] = 1 $$
with $ X_i $ drawn from $ p(.) $ which means that the average of our samples will converge to the expectation \cite[Chapter~2.4.1]{veach-thesis}.
From here we can formulate
\begin{equation*}
    \int_{a}^b g(x) * p(x) dx \stackrel{definition}{=} E(g(x))
    \stackrel{law~of~large~numbers}{\approx} \frac{1}{N} \sum_{i = 0}^N g(x_i)
\end{equation*}
and with $ g(x) = \frac{f(x)}{p(x)} $
\begin{equation*}
\begin{aligned}
    \int_{a}^b \frac{f(x)}{p(x)} p(x) dx &\approx \frac{1}{N} \sum_{i = 0}^N \frac{f(x)}{p(x)} \\
    \Leftrightarrow \int_{a}^b f(x) dx &\approx \frac{1}{N} \sum_{i = 0}^N \frac{f(x)}{p(x)}
\end{aligned}
\end{equation*}
which is basically the Monte Carlo equation \cite{pris}.

The last thing we want to introduce in this section is variance in the Monte Carlo integral.
Generally the variance is described by $$ V(X) = \frac{1}{N} \sum_{i = 0}^N (x_i - E(X))^2 $$ for discrete random variables \cite{pris}
and $$ V(X) = \int_{a}^b (x - I)^2 * p(x) dx $$ with $ I $ being the mean of the random variable $ X $ for continuous variables \cite{wyzant}.
When we look at the variance of our Monte Carlo integral we get
\begin{equation}
\label{eq:monte_carlo_variance}
\begin{aligned}
    V(g(x)) &= \int_{a}^b (g(x) - F)^2 * p(x) dx \\
            &= \int_{a}^b \left(\frac{f(x)}{p(x)} - F\right)^2 * p(x) dx
\end{aligned}
\end{equation}
with $ F $ being the integral from equation \ref{eq:integral_fx}.


\section{Importance Sampling}
\label{sec:importance_sampling}
With the variance from equation \ref{eq:monte_carlo_variance} let's see how we can improve it.
We can easily see that reducing the term $ \frac{f(x)}{p(x)} - F $ will finally reduce the variance the most since it will be exponentiated.
Since $ F $ and $ f(.) $ are fixed (that is the integral we want to calculate) only $ p(.) $ is left for modification.
When we choose $$ p(x) = \frac{f(x)}{\int_{a}^b f(x) dx} $$ we get
\begin{equation*}
\begin{aligned}
    \frac{f(x)}{p(x)} - F &= \frac{f(x)}{f(x)}\int_{a}^b f(x) dx - F \\
        &\stackrel{\ref{eq:integral_fx}}{=} \int_{a}^b f(x) dx - \int_{a}^b f(x) dx
        &= 0
\end{aligned}
\end{equation*}
which is perfect as we get a variance of 0 and are done.
The problem is that we don't know the integral at the start since that is what we want to calculate in the first place.
When we look at our chosen pdf we see that $ p(.) $ is proportional to $ f(.) $ by a factor $ \frac{1}{\int_{a}^b f(x) dx} = \frac{1}{F} := c $.
So we should at least try to chose $ p(.) $ somewhat proportional to $ f(.) $.
The difficulty is that $ f(.) $ can be arbitrary complex so we might only be able to approximate some part of it with one specific pdf.


\section{Multiple Importance Sampling}
\label{sec:multiple_importance_sampling}
Having multiple pdfs where each samples a different part of $ f(.) $ well
we would like to combine then to get the benefits from all of them.
\todo{Motivate with examples from rendering context}

\todo{Explain MIS in more detail than in the paper}
\todo{check what happends, when one technique says probability is 0}
